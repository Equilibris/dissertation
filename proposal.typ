#import "@preview/fletcher:0.5.8" as fletcher: diagram, node, edge
#import "@preview/timeliney:0.4.0"
#import "@preview/wordometer:0.1.5": word-count, total-words

#import "template.typ": *
// Take a look at the file `template.typ` in the file panel
// to customise this template and discover how it works.
#show: common.with(
  title: "Diss",
  author: "William Sørensen",
  abstract: lorem(59),
  acknowledgements: lorem(59),
  date: "April 29, 2023",
  college: "Gonville & Caius College",
  logo: "cst_logo.svg"
)

#show: word-count

#emph(par(justify:false, text(40pt)[Efficient coinductives through state-machine corecursors]))

*Supervisor:* Alex Keizer

*Marking supervisor:* /* Tobias Grosser OR */ Jamie Vicary

*Director of studies:* Russell Moore

*Word count:* #total-words

// Avigard implementation
// Split into 4 parts, Avigard, Me, Equiv, APp.

= Work to be undertaken

Previous work has shown how coinductive types can be encoded in Lean as quotients of polynomial functors (QPFs)@avigad_et_al.
// This encoding is expressive but inefficient for cofixed points.
This is done through approximation (@mtype).
For example accessing the $n$ index of a stream takes $cal(O)(n^2)$ time.
This becomes an issue when using Lean as a general purpose programming language.
An alternative approach is using a state-machine based encoding (@sme) of cofixed points.
This representation directly encodes the coalgebric structure and is well understood.
// The goal of this project is formalising the equivalence (@equiv) between these two representations.
// This equivalence should come directly from the corecursors of each of the implementations.
With these we get nice computational behaviour of the compiled code,
meaning directly getting a stream to depth $n$ is $cal(O)(n)$.
In @perf we can see in blue the performance from the current implementation,
versus in orange the performance of a prototype implementation the state-machine based version.

// A demonstration of the possible gains can be seen in @perf,
// the $x$-axis index and the $y$-axis is a duration.

#figure(
  image("MeanPerfWithSamples.png", width: 300pt),
  caption: [Graph plotting current performance v possible gains]
)<perf>

= Core project

This project will be implementing the state-machine encoding (@sme) of coinductive types,
and formalising the equivalence between these two engodings (@equiv).
I will start with the special example of equivalence between `Stream`s in the two representations,
then building up to the general cofix structure.

After this I will be implementing a coinductive monad allowing us to encode non-termination as an effect using the state-machine representation.
This will be an instance of the two representations.

= Starting point <startpoint>

I have worked with QPFs on the meta-programming side for an internship between my Part Ia and Part Ib.
During this I learnt of the basics of polynomial functors, and `TypeVec`s.
This means I am aware of what the underlying structures are when it comes to the raw implementation.
I have also done a feasibility assessment of the project by seeing how the current polynomials respond to universe levels.
This lead to me making 2 pull requests (\#28095, \#28279) to mathlib on `TypeVec` in preparation for my project.
I also tried to merge (\#28112) an implementation of commutable Shrink to mathlib that later got reverted when it was found to be inconsistent.

There are more minor refactoring pull requests towards the mathlib repository which don't change any behaviour but in general all of these can be
#link("https://github.com/leanprover-community/mathlib4/issues?q=author%3AEquilibris%20created%3A%3C2025-10-10")[#emph[found on this link]].
I Include these for completeness and transparency.

= Substance

// Add the structure

// There are a few structures that will be worked with during this project.
// `Shrink` (@shrk) is only used in the extension @shrkext

// An attempt was made to make `Shrink` commutable by using `unsafeCasts` but these had to be reverted.
// For now the exact type is undecided as there is a RFC by the Lean FRO adding repr types for this purpose.

@sme, @equiv and @ntmonad are all discuss structures that will be implemented throughout the project.
@mtype and @cfix both are implemented in Avigard et al @avigad_et_al.

== State-machine encoding <sme>

The state-machine encoding is the naïve way to implement the terminal coalgebra.
Given some polynomial functor $F$, the state-machine encoding is given by:
some type $alpha : "Type"$,
a function $f : alpha arrow.r F alpha$,
and some witness $a : alpha$.
With this you quotient over bisimilarity,
thereby only allowing direct usage of $"dest" : "Sme" F arrow.r F ("Sme" F)$

== The $M$ type<mtype>

The $M$ type is the name given to the terminal coalgebra of polynomial functors;
the possibly infinitely deep trees.
These are generated by progressive approximation where earlier trees must "agree" with the later ones.
Agreement is given by them being the same up to the previous depth.
A visual example is given in @agree.
We can have approximations for any n,
thereby letting the trees take any depth including infinite depth.
Trees can be terminated by having no children.
These have a corecursor $"corec" : {alpha : "Type"} arrow.r (f : alpha arrow.r F alpha) arrow.r alpha arrow.r M F$
and a destructurer $"dest" : "M" F arrow.r F (M F)$

#figure(
  diagram(cell-size:5mm, $
  0 edge("rrrrrrrrr", "..") & & & edge("lld") edge("d") edge("dr") & & & & edge("lld") edge("d") edge("dr") &
  \
  1 edge("rrrrrrrrr", "..") & & & & & edge("d") edge("dr") & & & edge("d")
  \
  2 edge("rrrrrrrrr", "..")
  $),
  caption:[Agreement of two trees of height 1 and 2]
)<agree>

// The current QPF implementation has 2 `M` types;
// univariate and multivariate implementations.
// These both have the undesired computational behaviour.
// `M` types are polynomial.

== The `Cofix` type<cfix>

`Cofix` is the terminal coalgebra in QPFs,
// (possibly) infinitely big quotiented trees.
These are simply a quotient over $M$ types lifting the quotient from the source QPF.
// This is the slightly concerning part of the project and by far the highest risk section as working with `Quot` in Lean is a painful experience.

== The equivalence <equiv>

The equivalence between the current implementation of `Cofix` and $M$ and the state-machine representation is the core of the project.
The functions in both directions are given by $f : "corec"_"Avigard" "dest"_"Sme"$ and $f^(-1) : "corec"_"Sme" "dest"_"Avigard"$.
The difficulty comes from proving $f compose f^(-1) = bb(1)$ and $f^(-1) compose f = bb(1)$.
Simpler versions of this equivalence also exist for the simpler representation.

== The non-termination monad<ntmonad>

The non-termination monad is a simple example of a coinductive.
This will be useful for testing the state-machine representation's performance.
The structure of the monad as a coinductive has two constructors as seen in this psudocode:

```
coinductive NTMonad (A : Type)
  | val : A -> NTMonad A
  | tau : NTMonad A -> NTMonad A
```

// == `Shrink`<shrk>

// `Shrink` is temporarily the choice used for doing the ABI translation between the two implementations.
// Given that the two types are equivalent then we can non-computably extract a model in the desired universe.
// Given two types $alpha : "Type" u$ and $beta : "Type" v$ for which an isomorphism exists,
// we can construct the type $"Shrink" alpha beta : "Type" v$ for which both diagrams in @shrkops commute.

// #figure(
//   diagram(cell-size: 15mm, $
//       alpha
//         edge("m" "k", ->)
//         edge("dr", bb(1)_alpha, ->)
//       & "Shrink" alpha beta
//         edge("d", "d" "e" "s" "t", ->, label-side: #left)
//       & "Shrink" alpha beta
//         edge("d", "d" "e" "s" "t", ->, label-side: #left)
//         edge("dr", bb(1)_("Shrink" alpha beta), ->)
//       \
//       & alpha
//       & alpha
//         edge("m" "k", ->)
//       & "Shrink" alpha beta
//   $),
//   caption:[Operations on Shrink]
// )<shrkops>

= Evaluation

The success of this project can be given by how close to the theorised performance we can get to.
The goal would be getting to the same order or magnitude.

= Core timeline

#timeliney.timeline(
  show-grid: true,
  {
    import timeliney: *

    headerline(
      group(([*Oct 25*], 4)),
      group(([*Nov 25*], 4)),
      group(([*Dec 25*], 4)),
      group(([*Jan 26*], 4)),
      group(([*Feb 26*], 4)),
      group(([*Mar 26*], 4))
    )

    headerline(
      ([11], 1), ([18], 1), ([25], 1),
      ([1 ], 1), ([8 ], 1), ([15], 1),
      ([22], 1), ([29], 1), ([6 ], 1),
      ([13], 1), ([20], 1), ([27], 1),
      ([3 ], 1), ([10], 1), ([17], 1),
      ([24], 1), ([31], 1), ([7 ], 1),
      ([14], 1), ([21], 1), ([28], 1),
      ([7 ], 1), ([14], 1), ([21], 1),
    )

    taskgroup(
      title: [*Core Work*],
      {
        task("Variable universe M's", (from: 0, to: 2, style: (stroke: blue)))
        task("Stream special example", (from: 2, to: 4, style: (stroke: green)))
        task("Univariate M", (from: 4, to: 9, style: (stroke: orange)))
        task("Multivariate M", (from: 9, to: 13, style: (stroke: red)))
        task("Cofix", (from: 13, to: 19, style: (stroke: purple)))
        task("NTMonad", (from: 19, to: 21, style: (stroke: gray)))
      }
    )

    taskgroup(
      title: [*Extensions*],
      {
        task("Shrink", (from: 21, to: 23, style: (stroke: gray)))
      }
    )

    milestone(
      at: 7,
      align(center, [
        *CAT Exam*\
        Dec 2025
      ])
    )
  }
)

The plan for work would be divided into a few different stages.

== Variable universe `M`s (2025-10-11 2w 2025-10-24)

// Rephrase to just have these structures.
To begin with I have to make the corecursor universe polymorphic.
As stated in @startpoint I have most of this code written.
I will attempt to get this merged into mathlib,
but if this review process takes too long I will be working on my own branch.
// To begin, the pull requests created from before the start of the project will have to be completed and merged into `mathlib`.

== `Stream` special example (2025-10-25 2w 2025-11-7)

The first step would be implementing `Stream` under the two representations.
Then I will familiarise myself with using bisimilarity to formalise the equivalence.
This is a first step to just understand how these features work.

// Write that i am actually implemeting theme
// An early step would be to familiarise myself with using the bisimilarity features given by the `M` type.
// This shows feasibility to prove equivalences of two `M` types with a simple functor.

== Univariate `M` (2025-11-8 2w 2025-11-21 + (CAT exam) + 2025-12-05 3w 2025-12-23)

Implementing the univariate `M` type is the natural next step from this.
This will be the natural next step in difficulty.
The work should lay the groundwork for the next proofs.
// After a special example I would move over to the univariate example.
// This will be much easier than the multivariate case as I don't have to suffer with `TypeVec`s.

== Multivariate `M` (2025-12-26 4w 2026-01-23)

The next step will be generalising the univariate implementation,
the same structure of the proof should be the same for the multivariate case.
The main difficulty here comes from working with `TypeVec`s.
These are quite difficult to reason about.

== `Cofix` (2026-01-24 6w 2026-03-06)

Finally, the `Cofix` type has to be implemented.
This will be the most challanging part as I will have to work with quotients.

== Implementing the NTMonad (2026-03-07 2w 2026-03-20)

This will be a demonstration of all of the previous work.

= Extensions

== Shrinking the representations (2026-03-21 2w 2026-04-04)

This involves finding a sound way allowing the current theory to use the efficient representation.
The soundness of this will be difficult to reason about as it requires working with the Lean code generator.
Therefore I leave this as an extension.
//
// == A fast implementation of `Precoroutines` (2026-03-21 2w 2026-04-04)

// Precoroutines are a type Alex Keizer is interested in.
// They generalise interaction trees and other similar data-types useful for denotational purposes.
// These leverage some of the powers of QPFs.
// This should come for free from the prior the core.

= Resources

Access to the restricted side of the lab is needed for working with the greater team.

#bibliography("proposal.bib")

